{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of anomalies in train: 24\n",
      "no of anomalies in test: 5\n",
      "(204, 204)\n"
     ]
    }
   ],
   "source": [
    "#Removed weekends\n",
    "#got the average for each hour 0-1 [Mon-fri]=B_01, 1-2 [Mon-fri]=B_12, 2-3 [Mon-fri]=B_23 ... 23-24 [Mon-fri]=B2324\n",
    "#Now the data points, Lets say in 0-1 [Mon-fri] > 0.95*B_01 not an anomaly, else anomaly.  \n",
    "\n",
    "from statistics import mean\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from pydot import Dot, Edge, Node\n",
    "from automata.fa.dfa import DFA\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from distances import  custom_pd\n",
    "import itertools\n",
    "from time import time\n",
    "\n",
    "df=pd.read_pickle(\"C:/Users/bchan/PycharmProjects/test/CaseStudy2021/data/power_demand/labeled/whole/power_data.pkl\")\n",
    "\n",
    "#To start from 1st day of weekday and end on last friday and also to make it multiple of 96 \n",
    "df=df[480:34560]\n",
    "c0 = [e[0] for e in df]  \n",
    "c1 = [e[1] for e in df]\n",
    "\n",
    "#cwd0 contains data point for weekdays only\n",
    "df_weekday=[]\n",
    "weekday_size=480\n",
    "cwd0=[]\n",
    "for f in range(0,len(c0),672):\n",
    "    cwd0.extend(c0[f:f+480])\n",
    "\n",
    "# Create empty lists for abc and t values\n",
    "abc = [[] for _ in range(24)]\n",
    "t = [0] * 24\n",
    "label = []\n",
    "\n",
    "# abc contains power consumption of each hour eg: abc[0]-->00am-1am\n",
    "for j in range(0, len(cwd0), 96):\n",
    "    for i in range(24):\n",
    "        abc[i].extend(cwd0[j + i * 4:j + (i + 1) * 4])\n",
    "\n",
    "# t contains average power consumption of each hour\n",
    "for i in range(24):\n",
    "    t[i] = round(mean(abc[i]), 1)\n",
    "\n",
    "# Calculate label values\n",
    "for j in range(0, len(cwd0), 96):\n",
    "    for i in range(24):\n",
    "        for k in range(4):\n",
    "            if cwd0[j + i * 4 + k] > t[i] * 0.90 and cwd0[j + i * 4 + k] < t[i] * 1.4:\n",
    "                label.append(0)\n",
    "            else:\n",
    "                label.append(1)\n",
    "\n",
    "df_day=[]\n",
    "window_size=96\n",
    "for f in range(0,len(cwd0),window_size):\n",
    "    pow=[round(sum(cwd0[j:j+window_size]),1) for j in range(f,f+(window_size),window_size)]\n",
    "    label_sum=[round(sum(label[j:j+window_size]),1) for j in range(f,f+(window_size),window_size)]\n",
    "    for i in label_sum:\n",
    "        # number of single point anomaly of 96 to be allowed\n",
    "        label1=[0] if i <= 20 else [1]    \n",
    "    df_day.append(pow + label1)\n",
    "\n",
    "#split data into train and test\n",
    "#df_day contains power consumption along with labels, power_day contains power values only \n",
    "df_day_train, df_day_test = train_test_split(df_day ,random_state=None, test_size=0.2, shuffle=True)\n",
    "\n",
    "sorted_data=sorted(set(e[0] for e in df_day))\n",
    "train_data=[e[0] for e in df_day_train]\n",
    "train_label=[e[1] for e in df_day_train]\n",
    "print(f'no of anomalies in train: {sum(train_label)}')\n",
    "\n",
    "test_data=[e[0] for e in df_day_test]\n",
    "test_label=[e[1] for e in df_day_test]\n",
    "print(f'no of anomalies in test: {sum(test_label)}')\n",
    "\n",
    "def my_scaler(column,min_val,max_val):\n",
    "    range_val = sorted_data[-1]-sorted_data[0]\n",
    "    scaled_values = [round(((max_val - min_val)/range_val)*(i - sorted_data[0]) + min_val,1) for i in column]\n",
    "    return scaled_values\n",
    "\n",
    "scaled_values=my_scaler(train_data,0,1)\n",
    "scaled_values1=my_scaler(test_data,0,1)\n",
    "\n",
    "converted_num = [str(i) for i in scaled_values]\n",
    "converted_num1 = [str(i) for i in scaled_values1]\n",
    "Lst=converted_num\n",
    "Lst1=converted_num1\n",
    "FL = [l.split(',') for l in Lst]\n",
    "FL_test = [l.split(',') for l in Lst1]\n",
    "\n",
    "uniq_list=set(Lst)\n",
    "alphabet = set(item for string in uniq_list for item in string.split(\",\") if item != ',')\n",
    "\n",
    "Pref_S = set()\n",
    "for string in uniq_list:\n",
    "    prefixes = string.split(',')\n",
    "    for i in range(1, len(prefixes) + 1):\n",
    "        prefix = ','.join(prefixes[:i])\n",
    "        Pref_S.add(prefix)\n",
    "Pref_S.add('')\n",
    "\n",
    "#compute the distance matrix\n",
    "dist = custom_pd(FL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n, eps, eps1, eps2):\n",
    "\n",
    "    #make changes after testing\n",
    "    #alphabet, Pref_S, Lst, FL, dist, df_day_test, my_scaler = data_preprocessW()\n",
    "    #alphabet, Pref_S, Lst, FL, dist, FL_test = data_preprocessW1()\n",
    "    #alphabet, Pref_S, Lst, FL, dist, FL_test = data_preprocessW()\n",
    "    \n",
    "    states = {str(f'q{i}') for i in range(n)}\n",
    "    start_state = 'q0'\n",
    "\n",
    "    env=gp.Env(empty=True)\n",
    "    env.setParam('OutputFlag', 0)\n",
    "    env.start()\n",
    "    \n",
    "    #t0 = time()\n",
    "    # Creating a new model\n",
    "    mpdad = gp.Model(\"DFA_PDAD\", env=env)\n",
    "\n",
    "    t0 = time()\n",
    "\n",
    "    #DECISION VARIABLES\n",
    "    delta = mpdad.addVars(states, alphabet, states, vtype=gp.GRB.BINARY, name='delta')\n",
    "    x = mpdad.addVars(Pref_S, states, vtype=gp.GRB.BINARY, name='x')\n",
    "    f = mpdad.addVars(states, vtype=gp.GRB.BINARY, name='f')\n",
    "    alpha = mpdad.addVars(len(Lst), states, vtype=gp.GRB.BINARY, name= 'alpha')\n",
    "    beta = mpdad.addVars(len(Lst), states, vtype=gp.GRB.BINARY, name= 'beta')\n",
    "\n",
    "    #OBJECTIVE\n",
    "    lambda_nn = len(Lst)*(len(Lst)-1)*np.max(dist)\n",
    "    print(f'eps: {eps}, eps1: {eps1}, eps2: {eps2}')\n",
    "    mpdad.setObjective(sum(beta[i,state1]*beta[k, state2]*dist[i,k]*(eps/lambda_nn) for i,_ in enumerate(Lst) for state1 in states for k,_ in enumerate(Lst) for state2 in states if dist[i,k] != 0) \\\n",
    "                    + sum(alpha[k,state2]*(eps1/len(Lst)) for k,_ in enumerate(Lst) for state2 in states) \\\n",
    "                    + sum(delta[state1,symbol,state2]*eps2 for state1 in states for symbol in alphabet for state2 in states if state1 != state2),\\\n",
    "                     gp.GRB.MINIMIZE)\n",
    "    \n",
    "    #AUTOMATA CONSTRAINTS\n",
    "    #Constraint1\n",
    "    for state0 in states:\n",
    "        for symbol in alphabet:\n",
    "            mpdad.addConstr(sum(delta[state0,symbol,state1] for state1 in states)==1, name=f'delta[{state0},{symbol}]')\n",
    "\n",
    "    #Constraint2\n",
    "    for word in Pref_S:\n",
    "        mpdad.addConstr(sum(x[word,state1] for state1 in states)==1, name=f'x[{word}]')\n",
    "\n",
    "    #Constraint3\n",
    "    mpdad.addConstr(x['',start_state]==1, name='initial_state')\n",
    "\n",
    "    #Constraint4 \n",
    "    for state0, word, symbol, state1 in itertools.product(states, Pref_S, alphabet, states):\n",
    "        if (word + ',' + symbol) in Pref_S:\n",
    "            mpdad.addConstr(x[word, state0] + delta[state0, symbol, state1] - 1 <= x[word + ',' + symbol, state1], name=f'transition[{state0},{word},{symbol},{state1}]')\n",
    "        if word == '' and symbol in Pref_S:\n",
    "            mpdad.addConstr(x[word, state0] + delta[state0, symbol, state1] - 1 <= x[symbol, state1], name=f'transition[{state0},{word},{symbol},{state1}]')\n",
    "\n",
    "\n",
    "    #BOUND CONSTRAINTS\n",
    "    for i, word in enumerate(Lst):\n",
    "        for state1 in states:\n",
    "            mpdad.addConstr(alpha[i, state1] >= x[word,state1] + f[state1] -1, name=f'bound_1[{state1},{i}]')        \n",
    "\n",
    "    for i, word in enumerate(Lst):\n",
    "        for state1 in states:\n",
    "            mpdad.addConstr(alpha[i, state1] <= x[word,state1], name=f'bound_2[{state1},{i}]')\n",
    "\n",
    "    for i, word in enumerate(Lst):\n",
    "        for state1 in states:\n",
    "            mpdad.addConstr(alpha[i, state1] <= f[state1], name=f'bound_3[{state1},{i}]')\n",
    "    \n",
    "    #not valid MILP constraint\n",
    "    '''\n",
    "    for i, word in enumerate(Lst):\n",
    "        for state1 in states:\n",
    "            mpdad.addConstr(x[word, state1] * (1-f[state1]) == beta[i, state1], name=f'bound_4[{state1},{i}]')\n",
    "    '''\n",
    "    for i, word in enumerate(Lst):\n",
    "        for state1 in states:\n",
    "            mpdad.addConstr(beta[i, state1] >= x[word,state1] + (1-f[state1]) -1, name=f'bound_5[{state1},{i}]')\n",
    "\n",
    "    for i, word in enumerate(Lst):\n",
    "        for state1 in states:\n",
    "            mpdad.addConstr(beta[i, state1] <= x[word,state1], name=f'bound_6[{state1},{i}]')\n",
    "\n",
    "    for i, word in enumerate(Lst):\n",
    "        for state1 in states:\n",
    "            mpdad.addConstr(beta[i, state1] <= (1-f[state1]), name=f'bound_7[{state1},{i}]')\n",
    "    \n",
    "    mpdad.optimize()\n",
    "    t1 = time()\n",
    "    print(\"Run time\", (t1-t0), \"seconds\")\n",
    "    mpdad.write(rf'C:\\Users\\bchan\\Desktop\\TUD\\Thesis\\model_PDAD_{n}.lp')\n",
    "\n",
    "    #print('Obj Value: %g' % mpdad.ObjVal)\n",
    "    #print(f\"Alpha: {mpdad.getAttr('X', alpha)}\")\n",
    "    #print(f\"Beta: {mpdad.getAttr('X', beta)}\")\n",
    "\n",
    "    if mpdad.status == 1:\n",
    "        status = 'LOADED'\n",
    "        print(f'DFAmodel_{n} LOADED')\n",
    "    \n",
    "    elif mpdad.status == 2:\n",
    "        print(f'DFAmodel_{n} OPTIMAL')\n",
    "        status='OPTIMAL'\n",
    "        transitions = mpdad.getAttr('X', delta)\n",
    "        t_values = [(s1,a,s2) for s1 in states for s2 in states for a in alphabet if round(transitions[s1, a, s2],0) == 1]\n",
    "        #for t in t_values:\n",
    "            #print(t)\n",
    "        f_s = mpdad.getAttr('X', f)\n",
    "        final_state = {s1 for s1 in states if round(f_s[s1],0) == 1}\n",
    "\n",
    "        transition_dict = create_transition_dict(states, alphabet, t_values)\n",
    "        #print(transition_dict)\n",
    "\n",
    "        dfa1 = DFA(states=states,input_symbols=alphabet, transitions= transition_dict, initial_state= start_state, final_states=final_state)\n",
    "        accepted = 0\n",
    "        rejected = 0\n",
    "        for w in FL:\n",
    "            if dfa1.accepts_input(w):\n",
    "                #print(f'{w}:accepted')\n",
    "                accepted += 1             \n",
    "            else:\n",
    "                #print(f'{w}:rejected')\n",
    "                rejected += 1        \n",
    "        print(f'Accepted in Training:{accepted}')\n",
    "        print(f'Rejected in Training:{rejected}')\n",
    "\n",
    "        create_diagram(rf'C:\\Users\\bchan\\Desktop\\TUD\\Thesis\\diagram_PDAD_{n}.png', states, start_state,final_state, transition_dict)\n",
    "        return dfa1, FL_test       \n",
    "    \n",
    "    elif mpdad.status == 3:\n",
    "        status = 'INFEASIBLE'\n",
    "        print(f'DFAmodel_{n} INFEASIBLE')\n",
    "    else:\n",
    "        print('status unknown, DEBUG!!')    \n",
    " \n",
    "    return status\n",
    "\n",
    "def create_transition_dict(states, alphabet, t_values):\n",
    "    transition_dict = {}\n",
    "\n",
    "    for state in states:\n",
    "        transition_dict[state] = {}\n",
    "        for symbol in alphabet:\n",
    "            transition_dict[state][symbol] = None\n",
    "\n",
    "    for trans in t_values:\n",
    "        current_state, symbol, next_state = trans\n",
    "        transition_dict[current_state][symbol] = next_state\n",
    "\n",
    "    return transition_dict\n",
    "\n",
    "def create_diagram(path, states, start_state, final_state, transition_dict):\n",
    "\n",
    "    graph = Dot(graph_type='digraph', rankdir='LR')\n",
    "    nodes = {}\n",
    "    for state in states:\n",
    "        if state == start_state:\n",
    "            # color start state with green\n",
    "            if state in final_state:\n",
    "                initial_state_node = Node(\n",
    "                    state,\n",
    "                    style='filled',\n",
    "                    peripheries=2,\n",
    "                    fillcolor='#66cc33')\n",
    "            else:\n",
    "                initial_state_node = Node(\n",
    "                    state, style='filled', fillcolor='#66cc33')\n",
    "            nodes[state] = initial_state_node\n",
    "            graph.add_node(initial_state_node)\n",
    "        else:\n",
    "            if state in final_state:\n",
    "                state_node = Node(state, peripheries=2)\n",
    "            else:\n",
    "                state_node = Node(state)\n",
    "            nodes[state] = state_node\n",
    "            graph.add_node(state_node)\n",
    "    # adding edges\n",
    "    for from_state, lookup in transition_dict.items():\n",
    "        for to_label, to_state in lookup.items():\n",
    "            graph.add_edge(Edge(\n",
    "                nodes[from_state],\n",
    "                nodes[to_state],\n",
    "                label=to_label\n",
    "            ))\n",
    "    if path:\n",
    "        graph.write_png(path)\n",
    "    return graph\n",
    "\n",
    "def test(dfa1, FL_test, test_label):\n",
    "    \n",
    "    accepted = 0\n",
    "    rejected = 0\n",
    "    Predicted_labels=[]\n",
    "    for w in FL_test:\n",
    "        #print(w)\n",
    "        if dfa1.accepts_input(w):\n",
    "            Predicted_labels.append(1)\n",
    "            #print('accepted')\n",
    "            accepted += 1             \n",
    "        else:\n",
    "            #print('rejected')\n",
    "            Predicted_labels.append(0)\n",
    "            rejected += 1  \n",
    "              \n",
    "    print(f'Accepted in Testing:{accepted}')\n",
    "    print(f'Rejected in Testing:{rejected}')\n",
    "    accuracy = accuracy_score(test_label, Predicted_labels)\n",
    "    print(f'Accuracy:{round(accuracy,2)}')\n",
    "    f1 = f1_score(test_label, Predicted_labels, average='binary', pos_label=1)\n",
    "    print(f'F1_score:{round(f1,2)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n denotes the number of states\n",
    "# eps, eps1, eps2 are regularization parameter\n",
    "for n in range(2,3):\n",
    "    for i, j, k in itertools.product([2,3,4], [1], [0]):\n",
    "        dfa1, FL_test = train(n, eps=i, eps1=j, eps2=k)\n",
    "        test(dfa1, FL_test, test_label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
